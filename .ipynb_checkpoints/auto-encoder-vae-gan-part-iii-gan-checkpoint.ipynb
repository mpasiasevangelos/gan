{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec0ad219",
   "metadata": {
    "id": "ad97osgu1CeW",
    "papermill": {
     "duration": 0.006487,
     "end_time": "2024-05-02T13:53:43.880352",
     "exception": false,
     "start_time": "2024-05-02T13:53:43.873865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Exploring Auto-encoders, Variational Auto-encoders and GANs for Images\n",
    "# Part III - Generative Adversarial Network\n",
    "\n",
    "In the excellent book _Hands on Machine Learning with Scikit-Learn, Keras, & Tensorflow_, Chapter 17 discusses autoencoders, variational autoencoders, and GANs.  The problems at the end of the chapter do not have answers provided, so I decided to try my hand at them.  Question 10 asks the reader to develop a variational autoencoder with an image dataset and use it to generate images.  Question 11 extends this to generating new images using a GAN.\n",
    "\n",
    "In [Part I](https://www.kaggle.com/code/kirkdco/auto-encoder-vae-gan-part-i-auto-encoder?scriptVersionId=97158111), I start by generating a frequentist auto-encoder.\n",
    "\n",
    "In [Part II](https://www.kaggle.com/code/kirkdco/auto-encoder-vae-gan-part-ii-vae), I develop a Variational Auto-encoder and show randomly generated images as well as imagest representing the average encoding for each class.\n",
    "\n",
    "In Part III, I will finish with a Generative Adversarial Network.\n",
    "\n",
    "You can find code for the book [here](https://github.com/ageron/handson-ml2), and my Github repository with my answers to exercises [here](https://github.com/KirkDCO/HandsOnML_Exercises).\n",
    "\n",
    "# Finding an image dataset\n",
    "For this exercise, I wanted to find an interesting image dataset that was not too complex but also not too simple.  A dataset with RGB images would also be nice.  After much searching through the [TensorFlow Datasets Catalog](https://www.tensorflow.org/datasets/catalog/overview) and [Kaggle Datasets](https://www.kaggle.com/datasets), I found the Kaggle [Flowers Dataset](https://www.kaggle.com/datasets/l3llff/flowers) consisting of 13,838 JPEG 96 x 96 images of 14 different flowers.  (The dataset changed with the addtion of a new flower type while I was working on this notebook, so the latest version may not match the numbers of files I've quoted or that you see in outputs, below.)  I added the dataset directly to this notebook from its location on Kaggle.\n",
    "\n",
    "For this notebook, the dataset was reduced to only sunflowers, providing 1027 images of sunflowers.  In initial tests, including all flowers proved to be too complex given the number of images available, however, reduction to a single flower type led to much better results.  Also, the sunflowers appeared to be the most consistent with respect to their image composition.  The original images are 96 x 96 RGB images, but they were reduced to 48 x 48.  After some experimentation, the 96 x 96 images did not perform well, and the 48 x 48 seemed better behaved.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d89148d",
   "metadata": {
    "id": "FfirIsoQ2Kk7",
    "papermill": {
     "duration": 0.004222,
     "end_time": "2024-05-02T13:53:43.892307",
     "exception": false,
     "start_time": "2024-05-02T13:53:43.888085",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "First step is to import needed packages and set up global variables, and create a generator for the flower images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3805f2b",
   "metadata": {
    "id": "xM5DN8UnTPIA",
    "papermill": {
     "duration": 6.33633,
     "end_time": "2024-05-02T13:53:50.232273",
     "exception": false,
     "start_time": "2024-05-02T13:53:43.895943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports and globals\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "K = keras.backend\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "BATCH_SIZE = 16 \n",
    "\n",
    "IMG_WIDTH  = 48 \n",
    "IMG_HEIGHT = 48 \n",
    "\n",
    "KERNEL_SIZE = 4\n",
    "ENCODING_SIZE = 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3b55277",
   "metadata": {
    "id": "GL7CsYdgTkkR",
    "outputId": "f44062d5-723f-4bd3-943f-dc64cb8364e8",
    "papermill": {
     "duration": 3.786907,
     "end_time": "2024-05-02T13:53:54.023013",
     "exception": false,
     "start_time": "2024-05-02T13:53:50.236106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1027 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# create a training data generator \n",
    "training_generator = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  r\"e:\\gan\\flowers\\sunflower\",\n",
    "  seed = 42,\n",
    "  image_size = (IMG_HEIGHT, IMG_WIDTH),\n",
    "  batch_size = BATCH_SIZE, \n",
    "  labels = None,\n",
    "  color_mode = 'rgb'\n",
    ")\n",
    "\n",
    "training_generator = training_generator.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d3de89",
   "metadata": {
    "id": "6iI-nwGr2QtL",
    "papermill": {
     "duration": 0.003486,
     "end_time": "2024-05-02T13:53:54.033863",
     "exception": false,
     "start_time": "2024-05-02T13:53:54.030377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GAN\n",
    "\n",
    "The GAN is based on the one presented in the book, but with a few modifications.\n",
    "\n",
    "* The generator and discriminator have been adjusted for RGB images.\n",
    "* Four convolutional layers are used, and the number of filters increases from 32 to 256.  I experimented with up to 4 convolutional layers and with more or less filters, but this seemed to produce the most reasonable results.\n",
    "* Batch normalization was added to the discriminator and the results were very poor, so the final model has this removed.\n",
    "* Dropout in the discriminator is critical.  Without it, the discriminator has a very hard time keeping pace with the generator.\n",
    "* ReLU activations were tried in the generator, but generally produced very poor results.  SeLU performed well.\n",
    "* A few experiments were done in which additional dense layers were added to the discriminator but these led to poorer results.\n",
    "* The size of the encodings seemed to be very sensitive to the depth of the network and the size of the images. \n",
    "\n",
    "The interdependence of various hyperparameters made it very difficult to find a good balance witin the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a3c7803",
   "metadata": {
    "id": "pxf_vUx6Urrg",
    "outputId": "dddd6bd9-cbcc-483a-a23c-d88a9014b5bd",
    "papermill": {
     "duration": 0.295225,
     "end_time": "2024-05-02T13:53:54.332661",
     "exception": false,
     "start_time": "2024-05-02T13:53:54.037436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 9216)              304128    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 6, 6, 256)        1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 12, 12, 128)      524416    \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 12, 12, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 24, 24, 64)       131136    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 24, 24, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 48, 48, 32)       32800     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 48, 48, 3)        1539      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 995,811\n",
      "Trainable params: 994,915\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 32)        1568      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 48, 48, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 64)        32832     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 128)       131200    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 6, 256)         524544    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 9216)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 9217      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 699,361\n",
      "Trainable params: 699,361\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 48, 48, 3)         995811    \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 1)                 699361    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,695,172\n",
      "Trainable params: 1,694,276\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = keras.Sequential([\n",
    "  keras.layers.Dense(256 * 6 * 6, activation = \"selu\", input_shape = [ENCODING_SIZE]),\n",
    "  keras.layers.Reshape([6, 6, 256]),\n",
    "  keras.layers.BatchNormalization(),\n",
    "  keras.layers.Conv2DTranspose(filters = 128, kernel_size = KERNEL_SIZE, strides = 2,\n",
    "                               padding = \"same\", activation = \"selu\",\n",
    "                               kernel_initializer='lecun_normal'),\n",
    "  keras.layers.BatchNormalization(),\n",
    "  keras.layers.Conv2DTranspose(filters = 64, kernel_size = KERNEL_SIZE, strides = 2,\n",
    "                               padding = \"same\", activation = \"selu\",\n",
    "                               kernel_initializer='lecun_normal'),\n",
    "  keras.layers.BatchNormalization(),\n",
    "  keras.layers.Conv2DTranspose(filters = 32, kernel_size = KERNEL_SIZE, strides = 2,\n",
    "                               padding = \"same\", activation = \"selu\",\n",
    "                               kernel_initializer='lecun_normal'),\n",
    "  keras.layers.Conv2DTranspose(filters = 3, kernel_size = KERNEL_SIZE, strides = 1,\n",
    "                               padding = 'same', activation = 'sigmoid')\n",
    "])\n",
    "generator.summary()   \n",
    "\n",
    "discriminator = keras.Sequential([\n",
    "  keras.layers.Conv2D(32, input_shape = [IMG_WIDTH, IMG_HEIGHT, 3], kernel_size = KERNEL_SIZE, \n",
    "                      strides = 1, padding = 'same', activation = keras.layers.LeakyReLU(0.2)),\n",
    "  keras.layers.Dropout(0.25),                      \n",
    "  keras.layers.Conv2D(64, kernel_size = KERNEL_SIZE, strides = 2,\n",
    "                      padding = 'same', activation = keras.layers.LeakyReLU(0.2)),\n",
    "  keras.layers.Dropout(0.25),                      \n",
    "  keras.layers.Conv2D(128, kernel_size = KERNEL_SIZE, strides = 2,\n",
    "                      padding = 'same', activation = keras.layers.LeakyReLU(0.2)),\n",
    "  keras.layers.Dropout(0.25),                      \n",
    "  keras.layers.Conv2D(256, kernel_size = KERNEL_SIZE, strides = 2,\n",
    "                      padding = 'same', activation = keras.layers.LeakyReLU(0.2)),\n",
    "  keras.layers.Dropout(0.25),                      \n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(1, activation = 'sigmoid')                                  \n",
    "])\n",
    "discriminator.summary()\n",
    "\n",
    "gan = keras.models.Sequential([generator, discriminator])\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31af1e68",
   "metadata": {
    "id": "QV7T9SOd4D_r",
    "papermill": {
     "duration": 0.003593,
     "end_time": "2024-05-02T13:53:54.340484",
     "exception": false,
     "start_time": "2024-05-02T13:53:54.336891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utility Functions\n",
    "\n",
    "Some utility functions from the book were used with some modifications.\n",
    "\n",
    "* In the _train_gain_ function, I added two accumulators with the actual image classes and the predicted classes from the discriminator.  This allowed me to add a confusion matrix and accuracy metric after each epoch to see how the discriminator performed.  This was very helpful to see when the discriminator was very poor and thus not providing good feedback to the generator.  \n",
    "* A plotting option was added to allow showing examples of generated (top row) and actual sunflowers (bottom row) every so often. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2897acc6",
   "metadata": {
    "id": "6YuRrd09YzHx",
    "papermill": {
     "duration": 0.026476,
     "end_time": "2024-05-02T13:53:54.370869",
     "exception": false,
     "start_time": "2024-05-02T13:53:54.344393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_multiple_images(images, n_cols=None):\n",
    "    n_cols = n_cols or len(images)\n",
    "    n_rows = (len(images) - 1) // n_cols + 1\n",
    "\n",
    "    if images.shape[-1] == 1:\n",
    "        images = np.squeeze(images, axis=-1)\n",
    "    plt.figure(figsize=(n_cols * 3, n_rows * 3))\n",
    "\n",
    "    for index, image in enumerate(images):\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(image, cmap=\"binary\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "def train_gan(gan, dataset, BATCH_SIZE, ENCODING_SIZE, n_epochs = 50, plot_frequency = 10, learning_rate = 0.01):\n",
    "    generator, discriminator = gan.layers\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, n_epochs))\n",
    "        preds = []\n",
    "        actuals = []\n",
    "        for X_batch in dataset:\n",
    "          # give some feedback during the epoch\n",
    "          print(\"=\", end = '')\n",
    "          X_batch /= 255\n",
    "\n",
    "          # phase 1 - training the discriminator\n",
    "          noise = tf.random.normal(shape=[len(X_batch), ENCODING_SIZE])\n",
    "          generated_images = generator(noise)\n",
    "          X_fake_and_real = tf.concat([generated_images, X_batch], axis=0)\n",
    "          y1 = tf.constant([[0.]] * len(X_batch) + [[1.]] * len(X_batch))\n",
    "          y1 += 0.05 * tf.random.uniform(tf.shape(y1)) # add random noise to labels - this seems to help training significantly\n",
    "          discriminator.trainable = True\n",
    "          discriminator.train_on_batch(X_fake_and_real, y1)\n",
    "\n",
    "          # capture the actual and predicted values for later\n",
    "          actuals.extend([y.numpy()[0] for y in y1])\n",
    "          preds.extend([y[0] for y in discriminator.predict(X_fake_and_real).tolist()])\n",
    "          \n",
    "          # phase 2 - training the generator\n",
    "          noise = tf.random.normal(shape=[len(X_batch), ENCODING_SIZE])\n",
    "          y2 = tf.constant([[1.]] * len(X_batch))\n",
    "          discriminator.trainable = False\n",
    "          gan.train_on_batch(noise, y2)\n",
    "\n",
    "        # print out a confusion matrix to see how the discriminator is doing\n",
    "        # commented out here to save space\n",
    "        print()\n",
    "        print(confusion_matrix([1 if a > 0.5 else 0 for a in actuals], \n",
    "                               [1 if p > 0.5 else 0 for p in preds]))\n",
    "        print('accuracy: ', accuracy_score([1 if a > 0.5 else 0 for a in actuals], \n",
    "                                           [1 if p > 0.5 else 0 for p in preds]))\n",
    "\n",
    "        # only plot every 10 epochs\n",
    "        if epoch % plot_frequency == 0:\n",
    "          plot_multiple_images(X_fake_and_real, 3)\n",
    "          plt.show()\n",
    "          \n",
    "    plot_multiple_images(X_fake_and_real, 3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b117b6de",
   "metadata": {
    "id": "FjPEAxOSppXk",
    "outputId": "8171c420-e3c5-401c-c044-70fa6deb4e8f",
    "papermill": {
     "duration": 339.222564,
     "end_time": "2024-05-02T13:59:33.597424",
     "exception": false,
     "start_time": "2024-05-02T13:53:54.374860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "="
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Exception encountered when calling layer \"batch_normalization\" \"                 f\"(type BatchNormalization).\n\n{{function_node __wrapped__FusedBatchNormV3_device_/job:localhost/replica:0/task:0/device:GPU:0}} cuDNN launch failure : input shape ([16,6,6,256]) [Op:FusedBatchNormV3]\n\nCall arguments received by layer \"batch_normalization\" \"                 f\"(type BatchNormalization):\n  • inputs=tf.Tensor(shape=(16, 6, 6, 256), dtype=float32)\n  • training=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m discriminator\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      7\u001b[0m gan\u001b[38;5;241m.\u001b[39mcompile(loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mRMSprop())\n\u001b[1;32m----> 9\u001b[0m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mENCODING_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_frequency\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 27\u001b[0m, in \u001b[0;36mtrain_gan\u001b[1;34m(gan, dataset, BATCH_SIZE, ENCODING_SIZE, n_epochs, plot_frequency, learning_rate)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# phase 1 - training the discriminator\u001b[39;00m\n\u001b[0;32m     26\u001b[0m noise \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mlen\u001b[39m(X_batch), ENCODING_SIZE])\n\u001b[1;32m---> 27\u001b[0m generated_images \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m X_fake_and_real \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([generated_images, X_batch], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     29\u001b[0m y1 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([[\u001b[38;5;241m0.\u001b[39m]] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_batch) \u001b[38;5;241m+\u001b[39m [[\u001b[38;5;241m1.\u001b[39m]] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_batch))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7208\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 7209\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInternalError\u001b[0m: Exception encountered when calling layer \"batch_normalization\" \"                 f\"(type BatchNormalization).\n\n{{function_node __wrapped__FusedBatchNormV3_device_/job:localhost/replica:0/task:0/device:GPU:0}} cuDNN launch failure : input shape ([16,6,6,256]) [Op:FusedBatchNormV3]\n\nCall arguments received by layer \"batch_normalization\" \"                 f\"(type BatchNormalization):\n  • inputs=tf.Tensor(shape=(16, 6, 6, 256), dtype=float32)\n  • training=None"
     ]
    }
   ],
   "source": [
    "# clear the session for a clean run\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "discriminator.compile(loss = 'binary_crossentropy', optimizer = keras.optimizers.RMSprop())\n",
    "discriminator.trainable = False\n",
    "gan.compile(loss = 'binary_crossentropy', optimizer = keras.optimizers.RMSprop())\n",
    "\n",
    "train_gan(gan, training_generator, BATCH_SIZE, ENCODING_SIZE, n_epochs = 100, plot_frequency = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb718ceb",
   "metadata": {
    "id": "KfFAYR_B5S_G",
    "papermill": {
     "duration": 0.090949,
     "end_time": "2024-05-02T13:59:33.780184",
     "exception": false,
     "start_time": "2024-05-02T13:59:33.689235",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "In early epochs, the generator essentially learns the color palette first.  In some examples, depending on the size of the images, the depth of the network, etc., the first few epoch were clearly noise in the wrong color space, but would quickly change to the right set of yellows and reds.  Within a few more epochs, a color separation in the images with blue on top, yellow in the middle, and darker (generally greenish) on the bottom, showing that the generator was learing the general structure of the images.  This structure typically consisted of a central, yellow sunflower, framed on a blue sky and having dark green leaves on the bottom.  Along the way, more circular shapes would begin to emerge, and eventually some images that had some sunflower characteristics were produced.  Granted, these will not fool anyone, but there's clearly a learning process happening.\n",
    "\n",
    "It was clear that larger images were much harder, and reducing the images to 48 x 48 helped.  Reducing the images further was helpful, but then the real images were themselves less convincing.  \n",
    "\n",
    "Deeper network with 4 or 5 convolutional layers and as many as 1024 filters in the deepest layer also improved the images, but only slightly.\n",
    "\n",
    "Clear limitations of this approach include:\n",
    "* there were only 1027 images in the real image set\n",
    "* the images were not very consistently framed - some have wonderfully centered flowers, but there are many camera angles, and many images of fields of sunflowers\n",
    "* much larger and more complex networks might help, but more training data will probably be more impactful\n",
    "\n",
    "Also interesting is the periodicity observed in the generated images.  There are repeated pattern blocks that are clearly reflective of the underlying network structure and weights.  I'm quite surprised at the degree of periodicity, however.  A more thorough investigation of the weights of each layer of the network would be interesting to determine why there are such periodic or symmetric structures."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1946928,
     "sourceId": 3551029,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30197,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 362.507068,
   "end_time": "2024-05-02T13:59:37.622497",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-02T13:53:35.115429",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
